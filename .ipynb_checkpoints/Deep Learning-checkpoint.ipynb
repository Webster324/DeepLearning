{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5e251d-8daa-44d3-81c8-c0e291ba2e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as TVF\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "data_dir = \"MY_data\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b395f14-38ff-4d65-8b05-bdf62dd6d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def augment_image(image, num_augmentations=2):\n",
    "    augmented_images = []\n",
    "    augment_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(40),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    ])\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        augmented_image = augment_transform(image)\n",
    "        augmented_images.append(augmented_image)\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "class AugmentedImageFolder(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, num_augmentations=2):\n",
    "        self.dataset = ImageFolder(root_dir, transform=None)  # Load without transform\n",
    "        self.transform = transform\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        for img, target in self.dataset:\n",
    "            self.data.append(img)  # img is already a PIL image\n",
    "            self.targets.append(target)\n",
    "            augmented_images = augment_image(img, self.num_augmentations)\n",
    "            for aug_img in augmented_images:\n",
    "                self.data.append(aug_img)\n",
    "                self.targets.append(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # Apply final transformation here\n",
    "        return img, target\n",
    "\n",
    "# Define the transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(40),                  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = AugmentedImageFolder(root_dir=data_dir + \"/train\", transform=train_transform, num_augmentations=50)\n",
    "\n",
    "train_size = int(0.8 * len(trainset)) \n",
    "val_size = len(trainset) - train_size \n",
    "\n",
    "trainSubset, valSubset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(trainSubset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(valSubset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b884c1a-2322-42cf-86c2-08e987e061ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset = ImageFolder(data_dir + \"/train\", transform=train_transform)\n",
    "#train_size = int(0.8 * len(trainset)) \n",
    "#val_size = len(trainset) - train_size \n",
    "\n",
    "#trainSubset, valSubset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "#train_loader = DataLoader(trainSubset, batch_size =64, shuffle=True)\n",
    "#val_loader = DataLoader(valSubset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8844226e-eec7-47a7-9b1a-dd1735f1ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plotting loss\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0b56a3a-cecc-4da4-a226-d56750995942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, valloader, device, num_epochs, lr=0.01, momentum=0.8, step_size=5, gamma=0.1, verbose=True):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    loss_iterations = int(np.ceil(len(trainloader) / 3))\n",
    "    \n",
    "    # Transfer model to GPU\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Set the optimizer using the lr and momentum settings passed by the user\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        net.train()  # Ensure the model is in training mode\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward propagation to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            # Backpropagation to get gradients of all parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy for the batch\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / total_samples\n",
    "        train_acc = running_corrects.float() / total_samples\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc.cpu().numpy())\n",
    "\n",
    "        # Validation phase\n",
    "        net.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0\n",
    "        val_corrects = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels)\n",
    "                val_samples += inputs.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc = val_corrects.double() / val_samples\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc.cpu().numpy())\n",
    "\n",
    "        print(f'[Epoch {epoch+1:2d}]: train_loss = {train_loss:.4f}, train_acc = {train_acc:.4f}, '\n",
    "              f'validation_loss = {val_loss:.4f}, validation_acc = {val_acc:.4f}')\n",
    "\n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc184b65-3af2-4240-b552-6d53bf57810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(net, dataloader, device):\n",
    "    net.eval()  # Set model to evaluation mode\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Extract the predicted class indices\n",
    "            \n",
    "            all_targets.extend(targets.cpu().numpy())  # Store the true labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())  # Store the predicted labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(np.array(all_predictions) == np.array(all_targets)) / len(all_targets)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1-Score\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31f38b87-5b27-4fb5-be28-d654a6d12b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CustomModel                              [64, 34]                  1,934,080\n",
       "├─Conv2d: 1-1                            [64, 16, 38, 38]          448\n",
       "├─Conv2d: 1-2                            [64, 32, 38, 38]          12,832\n",
       "├─Conv2d: 1-3                            [64, 32, 38, 38]          25,632\n",
       "├─BatchNorm2d: 1-4                       [64, 32, 38, 38]          64\n",
       "├─MaxPool2d: 1-5                         [64, 32, 19, 19]          --\n",
       "├─Conv2d: 1-6                            [64, 64, 19, 19]          18,496\n",
       "├─Conv2d: 1-7                            [64, 64, 19, 19]          102,464\n",
       "├─Conv2d: 1-8                            [64, 64, 19, 19]          102,464\n",
       "├─Conv2d: 1-9                            [64, 64, 19, 19]          36,928\n",
       "├─MaxPool2d: 1-10                        [64, 64, 9, 9]            --\n",
       "├─BatchNorm2d: 1-11                      [64, 64, 9, 9]            128\n",
       "├─Dropout: 1-12                          [64, 64, 9, 9]            --\n",
       "├─Conv2d: 1-13                           [64, 128, 9, 9]           73,856\n",
       "├─Conv2d: 1-14                           [64, 128, 9, 9]           147,584\n",
       "├─AdaptiveAvgPool2d: 1-15                [64, 128, 1, 1]           --\n",
       "├─BatchNorm2d: 1-16                      [64, 128, 1, 1]           256\n",
       "├─Linear: 1-17                           [64, 64]                  8,256\n",
       "├─BatchNorm1d: 1-18                      [64, 64]                  128\n",
       "├─Dropout: 1-19                          [64, 64]                  --\n",
       "├─Linear: 1-20                           [64, 34]                  2,210\n",
       "==========================================================================================\n",
       "Total params: 2,465,826\n",
       "Trainable params: 2,465,826\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 10.76\n",
       "==========================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 143.54\n",
       "Params size (MB): 2.13\n",
       "Estimated Total Size (MB): 146.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=34):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv5_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv5_3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv5_4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv5_5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5_6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.dropout2 = nn.Dropout(0.6)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)  \n",
    "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv2_1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv4_1(x))\n",
    "        x = F.relu(self.conv3_1(x))\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv5_6(x))\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        x = self.batchnorm3(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "    \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "num_classes = 10 \n",
    "net = CustomModel(num_classes=num_classes)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(net, input_size=(64, 3, 40, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ada8674-f1d9-420a-a6dd-9ced6bc7ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=13, kernel_size=5, padding=1)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.pool1 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(in_channels=13, out_channels=27, kernel_size=5, padding=1)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.pool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "#         #6250\n",
    "#         self.fc1 = nn.Linear(1323, 3100)\n",
    "\n",
    "#         self.act3 = nn.ReLU()\n",
    "        \n",
    "#         self.fc2 = nn.Linear(3100, 400)\n",
    "#         self.act4 = nn.Sigmoid()\n",
    "        \n",
    "#         self.fc3 = nn.Linear(400, 10)\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = self.conv1(x)\n",
    "#         x = self.act1(x)\n",
    "#         x = self.pool1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.act2(x)\n",
    "#         x = self.pool2(x)  \n",
    "        \n",
    "        \n",
    "#         x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.act3(x)\n",
    "        \n",
    "#         x = self.fc2(x)\n",
    "#         x = self.act4(x)\n",
    "        \n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# net = NeuralNetwork()\n",
    "# summary(net, input_size=(64, 3, 40, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2a1e6d0-c863-4781-b136-109cf214fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1]: train_loss = 2.3164, train_acc = 0.1023, validation_loss = 2.3216, validation_acc = 0.0985\n",
      "[Epoch  2]: train_loss = 2.3205, train_acc = 0.1007, validation_loss = 2.3120, validation_acc = 0.0905\n",
      "[Epoch  3]: train_loss = 2.3188, train_acc = 0.0989, validation_loss = 2.3078, validation_acc = 0.0978\n",
      "[Epoch  4]: train_loss = 2.3137, train_acc = 0.1039, validation_loss = 2.3333, validation_acc = 0.0978\n",
      "[Epoch  5]: train_loss = 2.3114, train_acc = 0.1088, validation_loss = 2.3085, validation_acc = 0.1122\n",
      "[Epoch  6]: train_loss = 2.2854, train_acc = 0.1291, validation_loss = 2.2769, validation_acc = 0.1571\n",
      "[Epoch  7]: train_loss = 2.2793, train_acc = 0.1505, validation_loss = 2.2751, validation_acc = 0.1615\n",
      "[Epoch  8]: train_loss = 2.2765, train_acc = 0.1503, validation_loss = 2.2705, validation_acc = 0.1636\n",
      "[Epoch  9]: train_loss = 2.2729, train_acc = 0.1447, validation_loss = 2.2665, validation_acc = 0.1586\n",
      "[Epoch 10]: train_loss = 2.2694, train_acc = 0.1456, validation_loss = 2.2641, validation_acc = 0.1651\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m train(net, train_loader, val_loader, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, trainloader, valloader, device, num_epochs, lr, momentum, step_size, gamma, verbose)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m     22\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Ensure the model is in training mode\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Transfer data to GPU\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mAugmentedImageFolder.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[idx]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 46\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)  \u001b[38;5;66;03m# Apply final transformation here\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, target\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:468\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    466\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    467\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mtuple\u001b[39m(size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), interpolation)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2200\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2192\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2193\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2194\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2195\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2196\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2197\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2198\u001b[0m         )\n\u001b[1;32m-> 2200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mresize(size, resample, box))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train(net, train_loader, val_loader, device, num_epochs=30, lr=0.01, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc89691-b2ae-4b44-9e9e-d4ff693d5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6ddac-c386-40bf-8f3c-ec4b66aa3f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "testset = ImageFolder(data_dir + \"/Test\", transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "evaluate(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b6fc2-89f9-462d-b361-b07f088302cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, transform=None):\n",
    "    model.eval()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    img = transform(image)\n",
    "    \n",
    "    img = img.to(device)\n",
    "    \n",
    "    xb = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "        \n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    \n",
    "    return preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172f0c7-46c8-4cab-a5f1-b58a3bbec26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
