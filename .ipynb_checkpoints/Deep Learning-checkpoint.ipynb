{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b5e251d-8daa-44d3-81c8-c0e291ba2e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as TVF\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "data_dir = \"MY_data\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b395f14-38ff-4d65-8b05-bdf62dd6d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Number of images in augmented trainset: 9204\n"
     ]
    }
   ],
   "source": [
    "# Define multiple augmentation strategies\n",
    "aug1 = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "aug2 = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "aug3 = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the original dataset\n",
    "original_dataset = ImageFolder(data_dir + \"/train\", transform=aug1)\n",
    "\n",
    "# Create augmented datasets\n",
    "augmented_datasets = [\n",
    "    ImageFolder(data_dir + \"/train\", transform=aug1),\n",
    "    ImageFolder(data_dir + \"/train\", transform=aug2),\n",
    "    ImageFolder(data_dir + \"/train\", transform=aug3)\n",
    "]\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "full_dataset = ConcatDataset([original_dataset] + augmented_datasets)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_subset, val_subset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print dataset details\n",
    "num_classes = len(original_dataset.classes)\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Number of images in trainset: {len(full_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8844226e-eec7-47a7-9b1a-dd1735f1ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0b56a3a-cecc-4da4-a226-d56750995942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, valloader, device, num_epochs, lr=0.01, weight_decay = 0.8, step_size = 5, gamma = 0.1):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Transfer model to GPU\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Set the optimizer using the lr and momentum settings passed by the user\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        net.train()  # Ensure the model is in training mode\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward propagation to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation to get gradients of all parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy for the batch\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / total_samples\n",
    "        train_acc = running_corrects.float() / total_samples\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc.cpu().numpy())\n",
    "\n",
    "        # Validation phase\n",
    "        net.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0\n",
    "        val_corrects = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels)\n",
    "                val_samples += inputs.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc = val_corrects.double() / val_samples\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc.cpu().numpy())\n",
    "\n",
    "        print(f'[Epoch {epoch+1:2d}]: train_loss = {train_loss:.4f}, train_acc = {train_acc:.4f}, '\n",
    "              f'validation_loss = {val_loss:.4f}, validation_acc = {val_acc:.4f}')\n",
    "\n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc184b65-3af2-4240-b552-6d53bf57810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(net, dataloader, device):\n",
    "    net.eval()  # Set model to evaluation mode\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Extract the predicted class indices\n",
    "            \n",
    "            all_targets.extend(targets.cpu().numpy())  # Store the true labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())  # Store the predicted labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(np.array(all_predictions) == np.array(all_targets)) / len(all_targets)\n",
    "    print(f'Test accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1-Score\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31f38b87-5b27-4fb5-be28-d654a6d12b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimplifiedModel                          [64, 36]                  --\n",
       "├─Conv2d: 1-1                            [64, 32, 40, 40]          896\n",
       "├─MaxPool2d: 1-2                         [64, 32, 20, 20]          --\n",
       "├─Conv2d: 1-3                            [64, 64, 20, 20]          18,496\n",
       "├─BatchNorm2d: 1-4                       [64, 64, 20, 20]          128\n",
       "├─MaxPool2d: 1-5                         [64, 64, 10, 10]          --\n",
       "├─Conv2d: 1-6                            [64, 128, 10, 10]         73,856\n",
       "├─BatchNorm2d: 1-7                       [64, 128, 10, 10]         256\n",
       "├─Conv2d: 1-8                            [64, 256, 10, 10]         295,168\n",
       "├─BatchNorm2d: 1-9                       [64, 256, 10, 10]         512\n",
       "├─AdaptiveAvgPool2d: 1-10                [64, 256, 1, 1]           --\n",
       "├─SEBlock: 1-11                          [64, 256, 1, 1]           --\n",
       "│    └─Linear: 2-1                       [64, 16]                  4,112\n",
       "│    └─Linear: 2-2                       [64, 256]                 4,352\n",
       "├─Linear: 1-12                           [64, 64]                  16,448\n",
       "├─BatchNorm1d: 1-13                      [64, 64]                  128\n",
       "├─Dropout: 1-14                          [64, 64]                  --\n",
       "├─Linear: 1-15                           [64, 32]                  2,080\n",
       "├─BatchNorm1d: 1-16                      [64, 32]                  64\n",
       "├─Dropout: 1-17                          [64, 32]                  --\n",
       "├─Linear: 1-18                           [64, 36]                  1,188\n",
       "==========================================================================================\n",
       "Total params: 417,684\n",
       "Trainable params: 417,684\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.93\n",
       "==========================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 92.01\n",
       "Params size (MB): 1.67\n",
       "Estimated Total Size (MB): 94.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        squeeze = F.adaptive_avg_pool2d(x, 1).view(batch, channels)\n",
    "        excitation = F.relu(self.fc1(squeeze))\n",
    "        excitation = torch.sigmoid(self.fc2(excitation)).view(batch, channels, 1, 1)\n",
    "        return x * excitation\n",
    "\n",
    "class SimplifiedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedModel, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Squeeze-and-Excitation Block\n",
    "        self.se = SEBlock(in_channels=256)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 36)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(32)\n",
    "        self.dropout = nn.Dropout(0.3)  # Reduced dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        # Apply Squeeze-and-Excitation block\n",
    "        x = self.se(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "net = SimplifiedModel()\n",
    "summary(net, input_size=(64, 3, 40, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cf7cacc-4dae-43e0-ba24-3f00caada35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [64, 10]                  --\n",
       "├─Conv2d: 1-1                            [64, 16, 40, 40]          448\n",
       "├─Conv2d: 1-2                            [64, 32, 40, 40]          4,640\n",
       "├─BatchNorm2d: 1-3                       [64, 32, 40, 40]          64\n",
       "├─MaxPool2d: 1-4                         [64, 32, 20, 20]          --\n",
       "├─Conv2d: 1-5                            [64, 64, 20, 20]          18,496\n",
       "├─BatchNorm2d: 1-6                       [64, 64, 20, 20]          128\n",
       "├─MaxPool2d: 1-7                         [64, 64, 10, 10]          --\n",
       "├─Conv2d: 1-8                            [64, 128, 10, 10]         73,856\n",
       "├─BatchNorm2d: 1-9                       [64, 128, 10, 10]         256\n",
       "├─MaxPool2d: 1-10                        [64, 128, 5, 5]           --\n",
       "├─Dropout: 1-11                          [64, 128, 5, 5]           --\n",
       "├─Linear: 1-12                           [64, 256]                 819,456\n",
       "├─Dropout: 1-13                          [64, 256]                 --\n",
       "├─Linear: 1-14                           [64, 10]                  2,570\n",
       "==========================================================================================\n",
       "Total params: 919,914\n",
       "Trainable params: 919,914\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.52\n",
       "==========================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 104.99\n",
       "Params size (MB): 3.68\n",
       "Estimated Total Size (MB): 109.90\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Increase the number of filters\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)  # 32 -> 64\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # 64 -> 128\n",
    "        self.batchnorm1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # 128 -> 256\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)  # 256 -> 512\n",
    "        self.batchnorm3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dropout layer after convolutional layers\n",
    "        self.dropout_conv = nn.Dropout(p=0.5)  # Dropout after convolutional layers\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 5 * 5, 1024)  # Adjusted to match new conv4 filters\n",
    "        self.dropout_fc1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 10)  # More units in fc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.dropout_conv(x)  # Apply dropout to convolutional features\n",
    "        \n",
    "        x = x.view(-1, 512 * 5 * 5)  # Adjust to match the new filters\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = CNN()\n",
    "summary(net, input_size=(64, 3, 40, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2a1e6d0-c863-4781-b136-109cf214fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1]: train_loss = 1.9231, train_acc = 0.3228, validation_loss = 1.7399, validation_acc = 0.3585\n",
      "[Epoch  2]: train_loss = 1.6648, train_acc = 0.4164, validation_loss = 1.4489, validation_acc = 0.5133\n",
      "[Epoch  3]: train_loss = 1.5110, train_acc = 0.4709, validation_loss = 1.3310, validation_acc = 0.5177\n",
      "[Epoch  4]: train_loss = 1.4190, train_acc = 0.5131, validation_loss = 1.2838, validation_acc = 0.5606\n",
      "[Epoch  5]: train_loss = 1.3522, train_acc = 0.5373, validation_loss = 1.2999, validation_acc = 0.5551\n",
      "[Epoch  6]: train_loss = 1.2123, train_acc = 0.5909, validation_loss = 1.1057, validation_acc = 0.6219\n",
      "[Epoch  7]: train_loss = 1.1397, train_acc = 0.6152, validation_loss = 1.0800, validation_acc = 0.6350\n",
      "[Epoch  8]: train_loss = 1.0720, train_acc = 0.6443, validation_loss = 0.9615, validation_acc = 0.6768\n",
      "[Epoch  9]: train_loss = 1.0534, train_acc = 0.6467, validation_loss = 0.9694, validation_acc = 0.6605\n",
      "[Epoch 10]: train_loss = 1.0025, train_acc = 0.6622, validation_loss = 0.9190, validation_acc = 0.6855\n",
      "[Epoch 11]: train_loss = 0.9180, train_acc = 0.6943, validation_loss = 0.8245, validation_acc = 0.7241\n",
      "[Epoch 12]: train_loss = 0.8865, train_acc = 0.7051, validation_loss = 0.7965, validation_acc = 0.7355\n",
      "[Epoch 13]: train_loss = 0.8519, train_acc = 0.7190, validation_loss = 0.8225, validation_acc = 0.7192\n",
      "[Epoch 14]: train_loss = 0.8368, train_acc = 0.7246, validation_loss = 0.7906, validation_acc = 0.7404\n",
      "[Epoch 15]: train_loss = 0.7878, train_acc = 0.7346, validation_loss = 0.7838, validation_acc = 0.7490\n",
      "[Epoch 16]: train_loss = 0.7639, train_acc = 0.7497, validation_loss = 0.6629, validation_acc = 0.7876\n",
      "[Epoch 17]: train_loss = 0.7347, train_acc = 0.7578, validation_loss = 0.6647, validation_acc = 0.7724\n",
      "[Epoch 18]: train_loss = 0.7080, train_acc = 0.7672, validation_loss = 0.6453, validation_acc = 0.7773\n",
      "[Epoch 19]: train_loss = 0.6982, train_acc = 0.7671, validation_loss = 0.6302, validation_acc = 0.7920\n",
      "[Epoch 20]: train_loss = 0.6905, train_acc = 0.7739, validation_loss = 0.6420, validation_acc = 0.7887\n",
      "[Epoch 21]: train_loss = 0.6525, train_acc = 0.7885, validation_loss = 0.5971, validation_acc = 0.8061\n",
      "[Epoch 22]: train_loss = 0.6164, train_acc = 0.7972, validation_loss = 0.6045, validation_acc = 0.7979\n",
      "[Epoch 23]: train_loss = 0.6110, train_acc = 0.8016, validation_loss = 0.5801, validation_acc = 0.8197\n",
      "[Epoch 24]: train_loss = 0.6028, train_acc = 0.8008, validation_loss = 0.5880, validation_acc = 0.8066\n",
      "[Epoch 25]: train_loss = 0.5902, train_acc = 0.8047, validation_loss = 0.5388, validation_acc = 0.8180\n",
      "[Epoch 26]: train_loss = 0.5861, train_acc = 0.8115, validation_loss = 0.5652, validation_acc = 0.8061\n",
      "[Epoch 27]: train_loss = 0.5704, train_acc = 0.8126, validation_loss = 0.5400, validation_acc = 0.8224\n",
      "[Epoch 28]: train_loss = 0.5522, train_acc = 0.8256, validation_loss = 0.5563, validation_acc = 0.8278\n",
      "[Epoch 29]: train_loss = 0.5522, train_acc = 0.8198, validation_loss = 0.5242, validation_acc = 0.8338\n",
      "[Epoch 30]: train_loss = 0.5520, train_acc = 0.8199, validation_loss = 0.5093, validation_acc = 0.8392\n",
      "[Epoch 31]: train_loss = 0.5314, train_acc = 0.8297, validation_loss = 0.4835, validation_acc = 0.8436\n",
      "[Epoch 32]: train_loss = 0.5280, train_acc = 0.8315, validation_loss = 0.5187, validation_acc = 0.8338\n",
      "[Epoch 33]: train_loss = 0.5187, train_acc = 0.8301, validation_loss = 0.5021, validation_acc = 0.8338\n",
      "[Epoch 34]: train_loss = 0.5136, train_acc = 0.8329, validation_loss = 0.4918, validation_acc = 0.8533\n",
      "[Epoch 35]: train_loss = 0.5097, train_acc = 0.8354, validation_loss = 0.4861, validation_acc = 0.8408\n",
      "[Epoch 36]: train_loss = 0.5016, train_acc = 0.8410, validation_loss = 0.4725, validation_acc = 0.8446\n",
      "[Epoch 37]: train_loss = 0.5105, train_acc = 0.8374, validation_loss = 0.4711, validation_acc = 0.8403\n",
      "[Epoch 38]: train_loss = 0.4991, train_acc = 0.8464, validation_loss = 0.4623, validation_acc = 0.8593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m train(net, train_loader, val_loader, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "Cell \u001b[1;32mIn[50], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, trainloader, valloader, device, num_epochs, lr, weight_decay, step_size, gamma)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m     21\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Ensure the model is in training mode\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Transfer data to GPU\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:348\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets[dataset_idx][sample_idx]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train(net, train_loader, val_loader, device, num_epochs=60, lr=0.001, weight_decay=1e-4,step_size=5, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc89691-b2ae-4b44-9e9e-d4ff693d5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57e6ddac-c386-40bf-8f3c-ec4b66aa3f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7385\n",
      "Precision: 0.7380\n",
      "Recall: 0.7385\n",
      "F1-Score: 0.7340\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testset = ImageFolder(data_dir + \"/Test\", transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "evaluate(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7b6fc2-89f9-462d-b361-b07f088302cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, transform=None):\n",
    "    model.eval()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    img = transform(image)\n",
    "    \n",
    "    img = img.to(device)\n",
    "    \n",
    "    xb = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "        \n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    \n",
    "    return preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172f0c7-46c8-4cab-a5f1-b58a3bbec26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
