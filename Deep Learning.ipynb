{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5e251d-8daa-44d3-81c8-c0e291ba2e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as TVF\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import os\n",
    "\n",
    "data_dir = \"fruits-360-original-size\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ac4d46-6ab3-4d9a-a48a-5774fb03c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomRotation(10), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b884c1a-2322-42cf-86c2-08e987e061ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ImageFolder(data_dir + \"/Training\", transform=data_transform)\n",
    "testset = ImageFolder(data_dir + \"/Test\", transform=data_transform)\n",
    "\n",
    "train_size = int(0.8 * len(trainset)) \n",
    "val_size = len(trainset) - train_size \n",
    "\n",
    "trainSubset, valSubset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(trainSubset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(valSubset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310d2484-4c2c-4043-b24a-d736e15aabbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train(net, trainloader, valloader, testloader, device, num_epochs, lr=0.01, momentum=0.8, step_size=5, gamma=0.1, verbose=True):\n",
    "    \n",
    "    loss_iterations = int(np.ceil(len(trainloader) / 3))\n",
    "    \n",
    "    # Transfer model to GPU\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Set the optimizer using the lr and momentum settings passed by the user\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        net.train()  # Ensure the model is in training mode\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward propagation to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            # Backpropagation to get gradients of all parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy for the batch\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / total_samples\n",
    "        train_acc = running_corrects.double() / total_samples\n",
    "\n",
    "        # Validation phase\n",
    "        net.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0\n",
    "        val_corrects = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels)\n",
    "                val_samples += inputs.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc = val_corrects.double() / val_samples\n",
    "\n",
    "        # Test phase\n",
    "        test_loss = 0\n",
    "        test_corrects = 0\n",
    "        test_samples = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in testloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_corrects += torch.sum(preds == labels)\n",
    "                test_samples += inputs.size(0)\n",
    "\n",
    "        test_loss /= test_samples\n",
    "        test_acc = test_corrects.double() / test_samples\n",
    "\n",
    "        print(f'[Epoch {epoch+1:2d}]: train_loss = {train_loss:.4f}, train_acc = {train_acc:.4f}, '\n",
    "              f'validation_loss = {val_loss:.4f}, validation_acc = {val_acc:.4f}, '\n",
    "              f'test_loss = {test_loss:.4f}, test_acc = {test_acc:.4f}')\n",
    "\n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b56a3a-cecc-4da4-a226-d56750995942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, device):\n",
    "    net.eval()  # Set model to evaluation mode\n",
    "    running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Extract the predicted class indices\n",
    "            running_corrects += (predicted == targets).sum().item()  # Compare indices directly\n",
    "\n",
    "    acc = running_corrects / len(dataloader.dataset)\n",
    "\n",
    "    print(f'Accuracy: {acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ada8674-f1d9-420a-a6dd-9ced6bc7ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 131) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1_bn(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2_bn(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.conv3_bn(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.conv4_bn(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(-1, 256 * 6 * 6)\n",
    "        \n",
    "        x = F.dropout(F.relu(self.fc1(x)), training=self.training, p=0.5)\n",
    "        x = F.dropout(F.relu(self.fc2(x)), training=self.training, p=0.5)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b04280-9c8e-4725-a945-266d924f0240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=131, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a1e6d0-c863-4781-b136-109cf214fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1]: train_loss = 2.3209, train_acc = 0.3082, validation_loss = 0.8126, validation_acc = 0.6816, test_loss = 0.7712, test_acc = 0.7039\n",
      "[Epoch  2]: train_loss = 1.1706, train_acc = 0.6278, validation_loss = 0.3441, validation_acc = 0.8773, test_loss = 0.3400, test_acc = 0.8920\n",
      "[Epoch  3]: train_loss = 0.9839, train_acc = 0.6972, validation_loss = 0.3200, validation_acc = 0.8966, test_loss = 0.3121, test_acc = 0.9096\n",
      "[Epoch  4]: train_loss = 0.8837, train_acc = 0.7350, validation_loss = 0.1886, validation_acc = 0.9334, test_loss = 0.1957, test_acc = 0.9293\n",
      "[Epoch  5]: train_loss = 0.7156, train_acc = 0.7753, validation_loss = 0.1535, validation_acc = 0.9447, test_loss = 0.1397, test_acc = 0.9524\n",
      "[Epoch  6]: train_loss = 0.4396, train_acc = 0.8656, validation_loss = 0.0229, validation_acc = 0.9968, test_loss = 0.0213, test_acc = 0.9965\n",
      "[Epoch  7]: train_loss = 0.3099, train_acc = 0.8999, validation_loss = 0.0133, validation_acc = 0.9984, test_loss = 0.0138, test_acc = 0.9968\n",
      "[Epoch  8]: train_loss = 0.2812, train_acc = 0.9085, validation_loss = 0.0096, validation_acc = 1.0000, test_loss = 0.0100, test_acc = 0.9984\n",
      "[Epoch  9]: train_loss = 0.2483, train_acc = 0.9173, validation_loss = 0.0127, validation_acc = 0.9984, test_loss = 0.0133, test_acc = 0.9971\n",
      "[Epoch 10]: train_loss = 0.2463, train_acc = 0.9183, validation_loss = 0.0106, validation_acc = 0.9992, test_loss = 0.0114, test_acc = 0.9974\n",
      "[Epoch 11]: train_loss = 0.2069, train_acc = 0.9304, validation_loss = 0.0097, validation_acc = 1.0000, test_loss = 0.0101, test_acc = 0.9987\n",
      "[Epoch 12]: train_loss = 0.1980, train_acc = 0.9348, validation_loss = 0.0095, validation_acc = 0.9984, test_loss = 0.0089, test_acc = 0.9981\n",
      "[Epoch 13]: train_loss = 0.2014, train_acc = 0.9360, validation_loss = 0.0075, validation_acc = 1.0000, test_loss = 0.0080, test_acc = 0.9984\n",
      "[Epoch 14]: train_loss = 0.2124, train_acc = 0.9346, validation_loss = 0.0084, validation_acc = 1.0000, test_loss = 0.0096, test_acc = 0.9977\n",
      "[Epoch 15]: train_loss = 0.2090, train_acc = 0.9302, validation_loss = 0.0084, validation_acc = 0.9992, test_loss = 0.0091, test_acc = 0.9984\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, test_loader, device, num_epochs=15, lr=0.01, momentum=0.9, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d359cfa-d5ff-4bfe-8e28-2151f5e3ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7b6fc2-89f9-462d-b361-b07f088302cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = img.unsqueeze(0)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f5110-9e39-4f87-97c0-57518a7b6900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
