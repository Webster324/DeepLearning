{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b5e251d-8daa-44d3-81c8-c0e291ba2e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as TVF\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "data_dir = \"MY_data\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b395f14-38ff-4d65-8b05-bdf62dd6d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Number of images in augmented trainset: 9204\n"
     ]
    }
   ],
   "source": [
    "# Define multiple augmentation strategies\n",
    "aug1 = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "aug2 = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "aug3 = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the original dataset\n",
    "original_dataset = ImageFolder(data_dir + \"/train\", transform=aug1)\n",
    "\n",
    "# Create augmented datasets\n",
    "augmented_datasets = [\n",
    "    ImageFolder(data_dir + \"/train\", transform=aug1),\n",
    "    ImageFolder(data_dir + \"/train\", transform=aug2),\n",
    "    ImageFolder(data_dir + \"/train\", transform=aug3)\n",
    "]\n",
    "\n",
    "# Concatenate the original and augmented datasets\n",
    "full_dataset = ConcatDataset([original_dataset] + augmented_datasets)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_subset, val_subset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print dataset details\n",
    "num_classes = len(original_dataset.classes)\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Number of images in  trainset: {len(full_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8844226e-eec7-47a7-9b1a-dd1735f1ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    \n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0b56a3a-cecc-4da4-a226-d56750995942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, valloader, device, num_epochs, lr=0.01, weight_decay = 0.8, step_size = 5, gamma = 0.1):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Transfer model to GPU\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Set the optimizer using the lr and momentum settings passed by the user\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        net.train()  # Ensure the model is in training mode\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward propagation to get outputs\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation to get gradients of all parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate accuracy for the batch\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = running_loss / total_samples\n",
    "        train_acc = running_corrects.float() / total_samples\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc.cpu().numpy())\n",
    "\n",
    "        # Validation phase\n",
    "        net.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0\n",
    "        val_corrects = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs, labels in valloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels)\n",
    "                val_samples += inputs.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_acc = val_corrects.double() / val_samples\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc.cpu().numpy())\n",
    "\n",
    "        print(f'[Epoch {epoch+1:2d}]: train_loss = {train_loss:.4f}, train_acc = {train_acc:.4f}, '\n",
    "              f'validation_loss = {val_loss:.4f}, validation_acc = {val_acc:.4f}')\n",
    "\n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc184b65-3af2-4240-b552-6d53bf57810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(net, dataloader, device):\n",
    "    net.eval()  # Set model to evaluation mode\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)  # Extract the predicted class indices\n",
    "            \n",
    "            all_targets.extend(targets.cpu().numpy())  # Store the true labels\n",
    "            all_predictions.extend(predicted.cpu().numpy())  # Store the predicted labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(np.array(all_predictions) == np.array(all_targets)) / len(all_targets)\n",
    "    print(f'Test accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Calculate Precision, Recall, and F1-Score\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31f38b87-5b27-4fb5-be28-d654a6d12b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimplifiedModel                          [64, 36]                  --\n",
       "├─Conv2d: 1-1                            [64, 32, 40, 40]          896\n",
       "├─MaxPool2d: 1-2                         [64, 32, 20, 20]          --\n",
       "├─Conv2d: 1-3                            [64, 64, 20, 20]          18,496\n",
       "├─BatchNorm2d: 1-4                       [64, 64, 20, 20]          128\n",
       "├─MaxPool2d: 1-5                         [64, 64, 10, 10]          --\n",
       "├─Conv2d: 1-6                            [64, 128, 10, 10]         73,856\n",
       "├─BatchNorm2d: 1-7                       [64, 128, 10, 10]         256\n",
       "├─Conv2d: 1-8                            [64, 256, 10, 10]         295,168\n",
       "├─BatchNorm2d: 1-9                       [64, 256, 10, 10]         512\n",
       "├─AdaptiveAvgPool2d: 1-10                [64, 256, 1, 1]           --\n",
       "├─SEBlock: 1-11                          [64, 256, 1, 1]           --\n",
       "│    └─Linear: 2-1                       [64, 16]                  4,112\n",
       "│    └─Linear: 2-2                       [64, 256]                 4,352\n",
       "├─Linear: 1-12                           [64, 64]                  16,448\n",
       "├─BatchNorm1d: 1-13                      [64, 64]                  128\n",
       "├─Dropout: 1-14                          [64, 64]                  --\n",
       "├─Linear: 1-15                           [64, 32]                  2,080\n",
       "├─BatchNorm1d: 1-16                      [64, 32]                  64\n",
       "├─Dropout: 1-17                          [64, 32]                  --\n",
       "├─Linear: 1-18                           [64, 36]                  1,188\n",
       "==========================================================================================\n",
       "Total params: 417,684\n",
       "Trainable params: 417,684\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.93\n",
       "==========================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 92.01\n",
       "Params size (MB): 1.67\n",
       "Estimated Total Size (MB): 94.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, _, _ = x.size()\n",
    "        squeeze = F.adaptive_avg_pool2d(x, 1).view(batch, channels)\n",
    "        excitation = F.relu(self.fc1(squeeze))\n",
    "        excitation = torch.sigmoid(self.fc2(excitation)).view(batch, channels, 1, 1)\n",
    "        return x * excitation\n",
    "\n",
    "class SimplifiedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimplifiedModel, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(256)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Squeeze-and-Excitation Block\n",
    "        self.se = SEBlock(in_channels=256)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 36)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(32)\n",
    "        self.dropout = nn.Dropout(0.3)  # Reduced dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        # Apply Squeeze-and-Excitation block\n",
    "        x = self.se(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "\n",
    "net = SimplifiedModel()\n",
    "summary(net, input_size=(64, 3, 40, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3cf7cacc-4dae-43e0-ba24-3f00caada35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      [64, 10]                  --\n",
       "├─Conv2d: 1-1                            [64, 64, 40, 40]          1,792\n",
       "├─Conv2d: 1-2                            [64, 128, 40, 40]         73,856\n",
       "├─BatchNorm2d: 1-3                       [64, 128, 40, 40]         256\n",
       "├─MaxPool2d: 1-4                         [64, 128, 20, 20]         --\n",
       "├─Conv2d: 1-5                            [64, 256, 20, 20]         295,168\n",
       "├─BatchNorm2d: 1-6                       [64, 256, 20, 20]         512\n",
       "├─MaxPool2d: 1-7                         [64, 256, 10, 10]         --\n",
       "├─Conv2d: 1-8                            [64, 512, 10, 10]         1,180,160\n",
       "├─BatchNorm2d: 1-9                       [64, 512, 10, 10]         1,024\n",
       "├─MaxPool2d: 1-10                        [64, 512, 5, 5]           --\n",
       "├─Dropout: 1-11                          [64, 512, 5, 5]           --\n",
       "├─Linear: 1-12                           [64, 1024]                13,108,224\n",
       "├─Dropout: 1-13                          [64, 1024]                --\n",
       "├─Linear: 1-14                           [64, 10]                  10,250\n",
       "==========================================================================================\n",
       "Total params: 14,671,242\n",
       "Trainable params: 14,671,242\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 23.70\n",
       "==========================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 419.96\n",
       "Params size (MB): 58.68\n",
       "Estimated Total Size (MB): 479.87\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Increase the number of filters\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)  # 32 -> 64\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # 64 -> 128\n",
    "        self.batchnorm1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)  # 128 -> 256\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)  # 256 -> 512\n",
    "        self.batchnorm3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Dropout layer after convolutional layers\n",
    "        self.dropout_conv = nn.Dropout(p=0.5)  # Dropout after convolutional layers\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 5 * 5, 1024)  # Adjusted to match new conv4 filters\n",
    "        self.dropout_fc1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 10)  # More units in fc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.dropout_conv(x)  # Apply dropout to convolutional features\n",
    "        \n",
    "        x = x.view(-1, 512 * 5 * 5)  # Adjust to match the new filters\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = CNN()\n",
    "summary(net, input_size=(64, 3, 40, 40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1e6d0-c863-4781-b136-109cf214fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  1]: train_loss = 2.4614, train_acc = 0.2991, validation_loss = 1.7233, validation_acc = 0.3895\n",
      "[Epoch  2]: train_loss = 1.7499, train_acc = 0.3982, validation_loss = 1.5729, validation_acc = 0.4753\n",
      "[Epoch  3]: train_loss = 1.6304, train_acc = 0.4478, validation_loss = 1.6212, validation_acc = 0.4405\n",
      "[Epoch  4]: train_loss = 1.5281, train_acc = 0.4865, validation_loss = 1.4846, validation_acc = 0.5225\n",
      "[Epoch  5]: train_loss = 1.4342, train_acc = 0.5355, validation_loss = 1.1955, validation_acc = 0.6095\n",
      "[Epoch  6]: train_loss = 1.2074, train_acc = 0.5983, validation_loss = 1.0773, validation_acc = 0.6475\n",
      "[Epoch  7]: train_loss = 1.1398, train_acc = 0.6290, validation_loss = 0.9887, validation_acc = 0.6676\n",
      "[Epoch  8]: train_loss = 1.0823, train_acc = 0.6436, validation_loss = 0.9450, validation_acc = 0.6752\n",
      "[Epoch  9]: train_loss = 1.0258, train_acc = 0.6660, validation_loss = 0.9282, validation_acc = 0.6860\n",
      "[Epoch 10]: train_loss = 0.9527, train_acc = 0.6878, validation_loss = 0.8956, validation_acc = 0.7089\n",
      "[Epoch 11]: train_loss = 0.8305, train_acc = 0.7310, validation_loss = 0.7638, validation_acc = 0.7529\n",
      "[Epoch 12]: train_loss = 0.7631, train_acc = 0.7512, validation_loss = 0.7211, validation_acc = 0.7621\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_accuracies, val_accuracies = train(net, train_loader, val_loader, device, num_epochs=60, lr=0.001, weight_decay=1e-4,step_size=5, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc89691-b2ae-4b44-9e9e-d4ff693d5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57e6ddac-c386-40bf-8f3c-ec4b66aa3f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7385\n",
      "Precision: 0.7380\n",
      "Recall: 0.7385\n",
      "F1-Score: 0.7340\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "testset = ImageFolder(data_dir + \"/Test\", transform=test_transform)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "evaluate(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7b6fc2-89f9-462d-b361-b07f088302cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, transform=None):\n",
    "    model.eval()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    img = transform(image)\n",
    "    \n",
    "    img = img.to(device)\n",
    "    \n",
    "    xb = img.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        yb = model(xb)\n",
    "        \n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    \n",
    "    return preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172f0c7-46c8-4cab-a5f1-b58a3bbec26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
